{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read annual plans\n",
    "#plans_og = pd.read_json('annualPlans1.json')\n",
    "\n",
    "#read in proposals, will train model on this larger set\n",
    "proposals = pd.read_csv(\"proposals.csv\", encoding = 'cp1252')\n",
    "\n",
    "#will run it later on this target set\n",
    "plans = pd.read_csv(\"all_plans.csv\",  encoding = 'cp1252')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4695"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_long = proposals.melt()\n",
    "p_long.columns = ['type', 'text']\n",
    "\n",
    "\n",
    "#will use later\n",
    "plans['full_text']= plans['description'] + plans['response_1'] + plans['response_2'] + plans['response_3']\n",
    "plan_text = plans['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23475"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean up text with simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'regional', 'collaborative', 'proposes', 'an', 'evidence', 'based', 'system', 'for', 'increasing', 'stem', 'and', 'cte', 'teacher', 'preparation', 'to', 'meet', 'the', 'in', 'demand', 'jobs', 'and', 'address', 'shortages', 'in', 'the', 'field', 'of', 'education']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(p_long.text))\n",
    "data_test = list(sent_to_words(plan_text))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the bigram and trigram models\n",
    "# bigram = gensim.models.Phrases(data_words, min_count=3, threshold=50) # higher threshold fewer phrases.\n",
    "# trigram = gensim.models.Phrases(bigram[data_words], threshold=10)  \n",
    "\n",
    "# # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# # See trigram example\n",
    "# #print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_stop = {'consortium', 'college', 'district', 'county','member', \n",
    "            'members', 'regions','plans', 'also', 'region', 'regional', 'desert',\n",
    "             'institution', 'north', 'west', 'south', 'east', 'valley', 'palo', \n",
    "            'effort', 'bakersfield', 'use', 'glendale', 'plan', 'would', 'add',\n",
    "            'must', 'different', 'extremely', 'year', 'edu', 'http', 'edu', 'ne',\n",
    "            'college', 'state', 'use', 'allow', 'take', 'could', 'look', 'consortium', 'college', \n",
    "            'district', 'county','member', 'members', 'regions','plans', \n",
    "            'also', 'region', 'regional', 'desert','institution', 'north', 'west', 'south', 'east', \n",
    "            'valley', 'palo', 'effort','bakersfield', 'use', 'glendale', 'plan', 'would',\n",
    "            'add', 'must', 'different', 'extremely','year', 'edu', 'http', 'edu', 'ne', \n",
    "            'college', 'state', 'use', 'allow', 'take', 'could', 'look',\n",
    "            'plan', 'joshua', 'tree', 'pearson', 'vue', 'west', 'end', 'corridor',\n",
    "            'santa', 'clarita', 'valley','palo', 'verde','south', 'orange', 'county',\n",
    "            'salano', 'december', 'stanislaus', 'counties', 'marin', 's',\"'s'\",'marin',\n",
    "            'luis', 'obispo', 'education', 'butte'}\n",
    "\n",
    "stop_words = stop_words.union(new_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_test_nostops = remove_stopwords(data_test)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "data_test_bigrams = make_bigrams(data_test_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"C:/Users/Sarah Robinson/Miniconda3/lib/site-packages/spacy/data/en/en_core_web_sm-2.0.0\") #didn't work... wouldn't load?\n",
    "\n",
    "#trying nltk tagger instead\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['collaborative', 'propos', 'evidence_based', 'system', 'increase', 'stem', 'cte', 'teacher_preparation', 'meet', 'demand', 'job', 'address', 'shortage', 'field'], ['proposal', 'create', 'dynamic', 'entrepreneurial', 'certificate', 'program', 'partnership', 'entrepreneurial', 'organization', 'partner', 'college', 'accord', 'northern_california', 'small_business', 'development', 'center', 'small_business', 'start_up', 'area', 'placer', 'expect', 'increase', 'period', 'exist', 'small_business', 'project', 'expand', 'need', 'qualified_employee', 'period', 'accord', 'burning_glass', 'labor_market', 'study', 'demand', 'student', 'small_business', 'management', 'skill', 'increase', 'placer', 'next_year', 'period', 'minority', 'business', 'expect', 'increase', 'placer', 'proposal', 'prepare', 'student', 'business', 'development', 'opportunity']]\n"
     ]
    }
   ],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, \n",
    "                                allowed_postags=['NOUN','ADJ', 'VERB'])\n",
    "\n",
    "test_lemmatized = lemmatization(data_test_bigrams, allowed_postags = ['NOUN', 'ADJ','VERB'])\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the dictionary and corpus needed for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "i2w_test = corpora.Dictionary(test_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "test_texts = test_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "test_corpus = [i2w_test.doc2bow(test) for test in test_texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('address', 1),\n",
       "  ('collaborative', 1),\n",
       "  ('cte', 1),\n",
       "  ('demand', 1),\n",
       "  ('evidence_based', 1),\n",
       "  ('field', 1),\n",
       "  ('increase', 1),\n",
       "  ('job', 1),\n",
       "  ('meet', 1),\n",
       "  ('propos', 1),\n",
       "  ('shortage', 1),\n",
       "  ('stem', 1),\n",
       "  ('system', 1),\n",
       "  ('teacher_preparation', 1)]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23475"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sarah robinson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\ldamodel.py:804: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)\n"
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=25, \n",
    "                                           random_state=100,\n",
    "                                           update_every=2,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "#save model to disk\n",
    "model_file = datapath(\"model\")\n",
    "lda_model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(22, '0.010*\"transform_communitie\" + 0.008*\"peralta\" + 0.008*\"sophisticated\" + 0.007*\"instrumental\" + 0.007*\"readily_available\" + 0.006*\"registration\" + 0.006*\"dept\" + 0.005*\"engine\" + 0.005*\"hartnell\" + 0.005*\"would\"'), (24, '0.026*\"marketing\" + 0.024*\"cccco\" + 0.024*\"priority_sector\" + 0.018*\"outreach\" + 0.017*\"brand\" + 0.014*\"campaign\" + 0.011*\"element\" + 0.010*\"crc\" + 0.010*\"deploy\" + 0.010*\"canada\"'), (8, '0.022*\"care\" + 0.017*\"mental_health\" + 0.011*\"projection\" + 0.010*\"tactic\" + 0.010*\"competition\" + 0.009*\"ineffective\" + 0.009*\"medium\" + 0.008*\"drive\" + 0.007*\"vacancy\" + 0.007*\"specialty\"'), (20, '0.017*\"english\" + 0.015*\"biotechnology\" + 0.013*\"hospitality\" + 0.010*\"agriculture_water\" + 0.010*\"pathways_trust\" + 0.009*\"accreditation\" + 0.008*\"multiple_measure\" + 0.008*\"distribute\" + 0.007*\"cooperative\" + 0.007*\"diversify\"'), (5, '0.037*\"cluster\" + 0.033*\"list\" + 0.032*\"automotive\" + 0.016*\"vehicle\" + 0.014*\"control\" + 0.010*\"search\" + 0.009*\"mechanism\" + 0.008*\"san_bernardino\" + 0.007*\"food_safety\" + 0.006*\"rapidly_evolv\"'), (19, '0.034*\"fire\" + 0.015*\"public_safety\" + 0.015*\"fire_academy\" + 0.014*\"entry_point\" + 0.013*\"award\" + 0.012*\"training\" + 0.011*\"similar\" + 0.011*\"science\" + 0.010*\"retail_hospitality\" + 0.009*\"academy\"'), (7, '0.019*\"cyber_security\" + 0.015*\"space\" + 0.012*\"yuba\" + 0.011*\"world\" + 0.010*\"capability\" + 0.010*\"remote\" + 0.009*\"combine\" + 0.009*\"document\" + 0.009*\"round\" + 0.008*\"cohort\"'), (2, '0.022*\"population\" + 0.020*\"boards_chamber\" + 0.013*\"engineering\" + 0.011*\"power\" + 0.011*\"cross\" + 0.007*\"machine\" + 0.007*\"design\" + 0.007*\"read\" + 0.006*\"survey_conduct\" + 0.006*\"occupations_studied\"'), (16, '0.036*\"construction\" + 0.025*\"classroom\" + 0.019*\"agriculture\" + 0.018*\"achievement\" + 0.017*\"safety\" + 0.014*\"efficiency\" + 0.009*\"enough\" + 0.008*\"establishment\" + 0.007*\"least\" + 0.006*\"mendocino\"'), (15, '0.035*\"public\" + 0.016*\"middle_skill\" + 0.016*\"participant\" + 0.015*\"dual\" + 0.014*\"less\" + 0.013*\"low\" + 0.013*\"family\" + 0.011*\"equity\" + 0.010*\"organizations_call\" + 0.010*\"system_engag\"'), (1, '0.061*\"health\" + 0.031*\"qualified\" + 0.027*\"bay_area\" + 0.026*\"healthcare\" + 0.026*\"occupation\" + 0.015*\"expansion\" + 0.013*\"direction\" + 0.012*\"environmental\" + 0.012*\"show\" + 0.012*\"nurse\"'), (10, '0.024*\"developer\" + 0.018*\"mobile\" + 0.016*\"simulation\" + 0.016*\"drone\" + 0.010*\"production\" + 0.010*\"test\" + 0.010*\"schedule\" + 0.008*\"regulation\" + 0.007*\"paramedic\" + 0.007*\"approval\"'), (14, '0.032*\"advanced_manufactur\" + 0.028*\"invest\" + 0.024*\"necessary\" + 0.017*\"innovation\" + 0.016*\"incumbent_worker\" + 0.016*\"challenge\" + 0.015*\"manufacturing\" + 0.015*\"vision\" + 0.013*\"dean\" + 0.013*\"advance\"'), (3, '0.036*\"strategic\" + 0.029*\"future\" + 0.022*\"planning\" + 0.017*\"company\" + 0.016*\"define\" + 0.015*\"pipeline\" + 0.015*\"present\" + 0.014*\"single\" + 0.012*\"produce\" + 0.012*\"force\"'), (6, '0.046*\"business\" + 0.031*\"job\" + 0.030*\"placement\" + 0.023*\"point\" + 0.021*\"small_business\" + 0.016*\"connect\" + 0.016*\"planning\" + 0.015*\"logistic\" + 0.014*\"southern_border\" + 0.014*\"global_trade\"'), (23, '0.096*\"high_school\" + 0.050*\"school\" + 0.026*\"teacher\" + 0.022*\"decision\" + 0.019*\"high\" + 0.014*\"want\" + 0.011*\"dual_enrollment\" + 0.008*\"middle\" + 0.008*\"particular\" + 0.007*\"attain\"'), (17, '0.073*\"technology\" + 0.054*\"equipment\" + 0.031*\"facility\" + 0.026*\"current\" + 0.023*\"certification\" + 0.022*\"standard\" + 0.022*\"new\" + 0.021*\"lab\" + 0.020*\"use\" + 0.019*\"engagement_strengthen\"'), (9, '0.087*\"need\" + 0.057*\"sector\" + 0.037*\"demand\" + 0.037*\"job\" + 0.036*\"industry\" + 0.032*\"meet\" + 0.028*\"address\" + 0.024*\"workforce\" + 0.023*\"labor_market\" + 0.017*\"growth\"'), (18, '0.115*\"program\" + 0.078*\"cte\" + 0.066*\"support\" + 0.063*\"increase\" + 0.039*\"faculty\" + 0.019*\"fund\" + 0.018*\"enrollment\" + 0.015*\"additional\" + 0.014*\"number\" + 0.014*\"improve\"'), (4, '0.037*\"student\" + 0.023*\"career\" + 0.023*\"development\" + 0.020*\"program\" + 0.017*\"industry\" + 0.016*\"pathway\" + 0.015*\"community\" + 0.015*\"priority\" + 0.014*\"opportunity\" + 0.014*\"provide\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 20 topics\n",
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate: coherence score and visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.899176897818209\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.46251934446489257\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vizualize the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os \n",
    "pyLDAvis.enable_notebook()\n",
    "pd.options.display.max_colwidth = 10000\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds = 'tsne' )\n",
    "pyLDAvis.display(vis)\n",
    "pyLDAvis.save_html(vis, 'lda_vis.html')\n",
    "# lda_html = pyLDAvis.prepared_data_to_html(vis, template_type='notebook')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
