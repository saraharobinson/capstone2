{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "\n",
    "# pd.get_option('display.max_colwidth') #50\n",
    "# pd.set_option('display.max_colwidth',60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import annual plan descriptions & the name of the institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "institutions_df = pd.read_csv('institutions_table.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans = pd.read_csv('all_plans.csv', encoding = 'cp1252')\n",
    "\n",
    "plans['full_text']= plans['description'] + plans['response_1'] + plans['response_2'] + plans['response_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>institution_id</th>\n",
       "      <th>name</th>\n",
       "      <th>proposal_id</th>\n",
       "      <th>fund_id</th>\n",
       "      <th>year_id</th>\n",
       "      <th>description</th>\n",
       "      <th>response_1</th>\n",
       "      <th>response_2</th>\n",
       "      <th>response_3</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>341</td>\n",
       "      <td>Desert</td>\n",
       "      <td>4069</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>The Desert Regional Consortium has made a conc...</td>\n",
       "      <td>All AEBG allocations are approved unanimously ...</td>\n",
       "      <td>All AEBG funds from present and prior years ar...</td>\n",
       "      <td>The Desert Regional Consortium will have calen...</td>\n",
       "      <td>The Desert Regional Consortium has made a conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>364</td>\n",
       "      <td>North Orange</td>\n",
       "      <td>4092</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>Working together with all eight NOCRC Members ...</td>\n",
       "      <td>NOCR followed the established practices when a...</td>\n",
       "      <td>The carry-over funds will help support with th...</td>\n",
       "      <td>NOCRC will engage all eight consortium members...</td>\n",
       "      <td>Working together with all eight NOCRC Members ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>342</td>\n",
       "      <td>South Bay (El Camino)</td>\n",
       "      <td>4070</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>MISSION\\nThe South Bay Adult Education Consort...</td>\n",
       "      <td>The uncertainty of the future funding makes it...</td>\n",
       "      <td>The South Bay Adult Education Consortium remai...</td>\n",
       "      <td>The SBAEC consortium has begun a strategic pla...</td>\n",
       "      <td>MISSION\\nThe South Bay Adult Education Consort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>390</td>\n",
       "      <td>South Bay (Southwestern)</td>\n",
       "      <td>4118</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>Vision: The South Bay Adult Education Consorti...</td>\n",
       "      <td>The planned allocations for CUSD, SWC, and SUH...</td>\n",
       "      <td>We will focus remaining carryover funds at the...</td>\n",
       "      <td>A consortium-wide summit was held in Spring 20...</td>\n",
       "      <td>Vision: The South Bay Adult Education Consorti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>374</td>\n",
       "      <td>San Bernardino</td>\n",
       "      <td>4102</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>In keeping with the Collective Impact approach...</td>\n",
       "      <td>IAEC Board Members engage in ongoing data, pro...</td>\n",
       "      <td>IAEC Member Districts participate in regularly...</td>\n",
       "      <td>The IAEC Executive Board holds regular meeting...</td>\n",
       "      <td>In keeping with the Collective Impact approach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   institution_id                      name  proposal_id  fund_id  year_id  \\\n",
       "0             341                    Desert         4069        3     2019   \n",
       "1             364              North Orange         4092        3     2019   \n",
       "2             342     South Bay (El Camino)         4070        3     2019   \n",
       "3             390  South Bay (Southwestern)         4118        3     2019   \n",
       "4             374            San Bernardino         4102        3     2019   \n",
       "\n",
       "                                         description  \\\n",
       "0  The Desert Regional Consortium has made a conc...   \n",
       "1  Working together with all eight NOCRC Members ...   \n",
       "2  MISSION\\nThe South Bay Adult Education Consort...   \n",
       "3  Vision: The South Bay Adult Education Consorti...   \n",
       "4  In keeping with the Collective Impact approach...   \n",
       "\n",
       "                                          response_1  \\\n",
       "0  All AEBG allocations are approved unanimously ...   \n",
       "1  NOCR followed the established practices when a...   \n",
       "2  The uncertainty of the future funding makes it...   \n",
       "3  The planned allocations for CUSD, SWC, and SUH...   \n",
       "4  IAEC Board Members engage in ongoing data, pro...   \n",
       "\n",
       "                                          response_2  \\\n",
       "0  All AEBG funds from present and prior years ar...   \n",
       "1  The carry-over funds will help support with th...   \n",
       "2  The South Bay Adult Education Consortium remai...   \n",
       "3  We will focus remaining carryover funds at the...   \n",
       "4  IAEC Member Districts participate in regularly...   \n",
       "\n",
       "                                          response_3  \\\n",
       "0  The Desert Regional Consortium will have calen...   \n",
       "1  NOCRC will engage all eight consortium members...   \n",
       "2  The SBAEC consortium has begun a strategic pla...   \n",
       "3  A consortium-wide summit was held in Spring 20...   \n",
       "4  The IAEC Executive Board holds regular meeting...   \n",
       "\n",
       "                                           full_text  \n",
       "0  The Desert Regional Consortium has made a conc...  \n",
       "1  Working together with all eight NOCRC Members ...  \n",
       "2  MISSION\\nThe South Bay Adult Education Consort...  \n",
       "3  Vision: The South Bay Adult Education Consorti...  \n",
       "4  In keeping with the Collective Impact approach...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words from the descriptions and use a stemmer (eg: Snowball) to reduce words to stems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "new_stop = {'consortium', 'college', 'district', 'county','member', 'members', 'regions','plans', \n",
    "            'also', 'region', 'regional', 'desert','institution', 'north', 'west', 'south', 'east', \n",
    "            'valley', 'palo', 'effort','bakersfield', 'use', 'glendale', 'plan', 'would',\n",
    "            'add', 'must', 'different', 'extremely','year', 'edu', 'http', 'edu', 'ne', \n",
    "            'college', 'state', 'use', 'allow', 'take', 'could', 'look', 'three', 'budget',\n",
    "            'plan', 'joshua', 'tree', 'pearson', 'vue', 'west', 'end', 'corridor',\n",
    "            'santa', 'clarita', 'valley','palo', 'verde','south', 'orange', 'county',\n",
    "            'salano', 'december', 'stanislaus', 'counties', 'marin', 's',\"'s'\",'marin',\n",
    "            'luis', 'obispo', 'education', 'butte'}\n",
    "stop = stop.union(new_stop)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = plans['full_text']\n",
    "#nltk.download('punkt')\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in descriptions:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'descriptions', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 52223 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "#create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column\n",
    "\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, \n",
    "                           index = totalvocab_stemmed)\n",
    "print ('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use NLTK collocations package to find phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "\n",
    "token_text = [token for token in totalvocab_tokenized if token not in stop]\n",
    "bigramFinder = BigramCollocationFinder.from_words(token_text)\n",
    "\n",
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "trigrams = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('criminal', 'justice'),\n",
       " ('dsusd', 'psusd'),\n",
       " ('love', 'logic'),\n",
       " ('therapy', 'aide'),\n",
       " ('chamber', 'commerce'),\n",
       " ('mendocino', 'lake'),\n",
       " ('environmental', 'scan'),\n",
       " ('capital', 'outlay'),\n",
       " ('task', 'forces'),\n",
       " ('mother', 'lode')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramFinder.apply_freq_filter(3)\n",
    "bigramFinder.nbest(bigrams.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('long-term', 'trends', 'high-growth'),\n",
       " ('manage', 'spend-down', 'expenditure'),\n",
       " ('trends', 'high-growth', 'sectors'),\n",
       " ('innovation', 'opportunity', 'act'),\n",
       " ('foundational', 'toward', 'realization'),\n",
       " ('human', 'centered', 'design'),\n",
       " ('continuous', 'improvement', 'among'),\n",
       " ('spend-down', 'expenditure', 'reporting'),\n",
       " ('communities', 'blythe', 'needles'),\n",
       " ('serves', 'foundational', 'toward'),\n",
       " ('wioa', 'title', 'ii'),\n",
       " ('ensure', 'continuous', 'improvement'),\n",
       " ('opportunity', 'act', 'wioa'),\n",
       " ('create', 'stability', 'among'),\n",
       " ('exploration', 'job', 'search'),\n",
       " ('together', 'create', 'stability'),\n",
       " ('fund', 'due', 'expire'),\n",
       " ('analysis', 'ensure', 'continuous'),\n",
       " ('improvement', 'among', 'decisions'),\n",
       " ('decision', 'making', 'comprehensive')]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigramFinder.apply_freq_filter(3)\n",
    "trigramFinder.nbest(trigrams.pmi, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter out ngrams based on word type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams\n",
    "bigram_freq = bigramFinder.ngram_fd.items()\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "#trigrams\n",
    "trigram_freq = trigramFinder.ngram_fd.items()\n",
    "trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function to filter for ADJ/NN bigrams and filter out stopwords\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in stop or word.isspace():\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#filter bigrams\n",
    "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n",
    "#function to filter for trigrams\n",
    "def rightTypesTri(ngram):\n",
    "    if '-pron-' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords or word.isspace():\n",
    "            return False\n",
    "    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#filter trigrams\n",
    "filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only those with more than 20 occurences\n",
    "bigramFinder.apply_freq_filter(7)\n",
    "trigramFinder.apply_freq_filter(7)\n",
    "\n",
    "bigramPMITable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.pmi)), \n",
    "                              columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n",
    "trigramPMITable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.pmi)), \n",
    "                               columns=['trigram','PMI']).sort_values(by='PMI', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigram</th>\n",
       "      <th>PMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(english, second, language)</td>\n",
       "      <td>19.080425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(wioa, strong, workforce)</td>\n",
       "      <td>15.479315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(planned, allocations, consistent)</td>\n",
       "      <td>15.374052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(high, school, diploma)</td>\n",
       "      <td>14.908368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(carry-over, funds, prior)</td>\n",
       "      <td>13.564471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              trigram        PMI\n",
       "0         (english, second, language)  19.080425\n",
       "1           (wioa, strong, workforce)  15.479315\n",
       "2  (planned, allocations, consistent)  15.374052\n",
       "3             (high, school, diploma)  14.908368\n",
       "4          (carry-over, funds, prior)  13.564471"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigramPMITable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>PMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(mother, lode)</td>\n",
       "      <td>12.027215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(ml, ace)</td>\n",
       "      <td>11.834570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(los, angeles)</td>\n",
       "      <td>11.512642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(mini, grant)</td>\n",
       "      <td>10.375138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(due, expire)</td>\n",
       "      <td>10.249607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           bigram        PMI\n",
       "0  (mother, lode)  12.027215\n",
       "1       (ml, ace)  11.834570\n",
       "2  (los, angeles)  11.512642\n",
       "3   (mini, grant)  10.375138\n",
       "4   (due, expire)  10.249607"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramPMITable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SciKit-Learn's TFIDF Vectorizer to obtain a weighted list of term occurrences for each plan\n",
    "Also optionally use CountVectorizer to help see what's going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"C:/Users/Sarah Robinson/Miniconda3/lib/site-packages/spacy/data/en/en_core_web_sm-2.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    doc = nlp(\" \".join(texts)) \n",
    "    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['test', 'friend', 'nlp', 'shit', 'friggen']]\n"
     ]
    }
   ],
   "source": [
    "test_words = ['test', 'friend', 'very', 'good', 'at', 'this', 'nlp', 'shit', \n",
    "              'i', 'am', 'so', 'friggen', 'tyty']\n",
    "\n",
    "test_lemons = lemmatization(test_words, allowed_postags = ['NOUN'])\n",
    "print(test_lemons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stem_lem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    #filter all but noun, adjectives, and verbs\n",
    "    lemmons = lemmatization(tokens, allowed_postags = ['NOUN', 'VERB','ADJ'])\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in lemmons:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sarah robinson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", 'abov', 'ani', 'becaus', 'befor', 'butt', 'colleg', 'counti', 'decemb', 'differ', 'doe', 'dure', 'educ', 'extrem', 'glendal', 'institut', 'lui', 'might', \"n't\", 'need', 'onc', 'onli', 'orang', 'ourselv', 'sha', 'themselv', 'verd', 'veri', 'whi', 'wo', 'yourselv'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.56 s\n",
      "(71, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.6, max_features=200,\n",
    "                                 min_df=0.001, stop_words= stop,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,4))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(descriptions) #fit the vectorizer to descriptions\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vocabulary list\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use scikit learn to get the cosine distance between descriptions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71x200 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5280 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 205 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_ids = plans.name\n",
    "plans_dict = {'institution': institution_ids, 'description': descriptions, 'cluster':clusters}\n",
    "df = pd.DataFrame.from_dict(plans_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>institution</th>\n",
       "      <th>description</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Desert</td>\n",
       "      <td>The Desert Regional Consortium has made a conc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>North Orange</td>\n",
       "      <td>Working together with all eight NOCRC Members ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Bay (El Camino)</td>\n",
       "      <td>MISSION\\nThe South Bay Adult Education Consort...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Bay (Southwestern)</td>\n",
       "      <td>Vision: The South Bay Adult Education Consorti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>San Bernardino</td>\n",
       "      <td>In keeping with the Collective Impact approach...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                institution  \\\n",
       "0                    Desert   \n",
       "1              North Orange   \n",
       "2     South Bay (El Camino)   \n",
       "3  South Bay (Southwestern)   \n",
       "4            San Bernardino   \n",
       "\n",
       "                                         description  cluster  \n",
       "0  The Desert Regional Consortium has made a conc...        4  \n",
       "1  Working together with all eight NOCRC Members ...        4  \n",
       "2  MISSION\\nThe South Bay Adult Education Consort...        0  \n",
       "3  Vision: The South Bay Adult Education Consorti...        0  \n",
       "4  In keeping with the Collective Impact approach...        1  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    26\n",
      "4    23\n",
      "0    12\n",
      "1     8\n",
      "3     2\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['cluster'].value_counts())\n",
    "df.set_index('cluster', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: b'classes', b'sites', b'adults', b'team', b'well', b'instruction', b'building', b'activities', b'last', b'disabilities', b'help', b'exploration', b'noncredit', b'progress', b'market', b'project', b'original', b'basic', b'groups', b'consist',\n",
      "\n",
      "Cluster 0 instituions: South Bay (El Camino), South Bay (Southwestern), Butte-Glenn, Mid Alameda County (Chabot-Las Positas), Southeast Los Angeles, College of the Canyons, San Francisco, Monterey, Southern Alameda County (Ohlone), Salinas Valley, South Bay (San Jose Evergreen), Morongo Basin,\n",
      "\n",
      "Cluster 1 words: b'aep', b'three-year', b'fiscally', b'board', b'ensure', b'engage', b'program', b'county', b\"'s\", b'across', b'approved', b'academic', b'gaps', b'managed', b'comprehensive', b'noncredit', b'adults', b'make', b'approaches', b'ongoing',\n",
      "\n",
      "Cluster 1 instituions: San Bernardino, Victor Valley, South Orange, Ventura County, Feather River, Contra Costa, Santa Monica, Barstow,\n",
      "\n",
      "Cluster 2 words: b'county', b'training', b'classes', b'skills', b'3-year', b'learners', b'high', b'certificates', b'well', b'local', b'reviewed', b'within', b'diploma', b'adults', b'agencies', b'instruction', b'serves', b'market', b'current', b'learning',\n",
      "\n",
      "Cluster 2 instituions: Gavilan, Gateway (Merced), Sierra Joint, Imperial, Los Angeles, Mendocino-Lake, Rancho Santiago, North Coast, ACCEL (San Mateo), Marin, Northern Alameda County (Peralta), Napa Valley, Palo Verde, San Diego, Santa Cruz, Riverside About Students, Siskiyous, Kern, Stanislaus Mother Lode (Yosemite), Lassen, West Hills, West Kern, Sonoma, Capital (Los Rios), Pasadena, Shasta-Tehama-Trinity,\n",
      "\n",
      "Cluster 3 words: b'saec', b'objectives', b'committee', b'academic', b'access', b'strategic', b'building', b'retreat', b'program', b'staff', b'counseling', b'support', b'seamless', b'current', b'within', b'job', b'first', b'board', b'county', b'assist',\n",
      "\n",
      "Cluster 3 instituions: Sequoias, Solano,\n",
      "\n",
      "Cluster 4 words: b'3-year', b'created', b'initiated', b'three-year', b'progress', b'made', b'leveraging', b'staff', b'stakeholders', b'outcome', b'address', b'local', b'engage', b'effectiveness', b'reviewed', b'activities', b'ensure', b'san', b'approved', b'learning',\n",
      "\n",
      "Cluster 4 instituions: Desert, North Orange, Lake Tahoe, Mt. San Antonio, Coast, San Diego East (Grossmont-Cuyamaca), Rio Hondo, Coastal North, Long Beach, Foothill De Anza, Allan Hancock, State Center, Citrus, Santa Barbara, Southwest Riverside, Delta Sierra Alliance, San Diego North (Palomar), Tri-Cities, Antelope Valley, Glendale, West End Corridor, San Luis Obispo, North Central (Yuba),\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :20]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d instituions:\" % i, end='')\n",
    "    for ids in df.loc[i]['institution']:\n",
    "        print(' %s,' % ids, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
